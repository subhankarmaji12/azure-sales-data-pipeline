{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d037edc-2c00-4d58-9e51-15b6df4b8652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e2f017-097f-435c-bbf4-e576464e2024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5432627103706474>, line 15\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# # Unmount (only if you are sure)\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# dbutils.fs.unmount(\"/mnt/raw-data\")\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Then remount with new config\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n",
       "\u001B[1;32m     16\u001B[0m     source\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://transformed-data@crmstorageaccountyt.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     17\u001B[0m     mount_point\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/transformed-data\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     18\u001B[0m     extra_configs \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m     19\u001B[0m        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.crmstorageaccountyt.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     20\u001B[0m            dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricksScope\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecretskv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     21\u001B[0m     }\n",
       "\u001B[1;32m     22\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001B[0m, in \u001B[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    171\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o465.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:207)\n",
       "\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:105)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1387)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1413)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1407)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1093)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1093)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$27(DbfsServerBackend.scala:592)\n",
       "\tat com.databricks.backend.daemon.data.exceptionpayload.ExceptionPayloadSerializer$.serializeExceptionToDse(ExceptionPayloadSerializer.scala:35)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$26(DbfsServerBackend.scala:590)\n",
       "\tat com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:473)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:442)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n",
       "\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n",
       "\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n",
       "\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n",
       "\tat com.databricks.rpc.armeria.UnaryGrpcService.handleMessageInternal(UnaryGrpcService.scala:130)\n",
       "\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$3(UnaryGrpcService.scala:77)\n",
       "\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n",
       "\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$1(UnaryGrpcService.scala:77)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n",
       "\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n",
       "\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n",
       "\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n",
       "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
       "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ExecutionError",
        "evalue": "An error occurred while calling o465.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:207)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:105)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1387)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1413)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1407)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1093)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1093)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$27(DbfsServerBackend.scala:592)\n\tat com.databricks.backend.daemon.data.exceptionpayload.ExceptionPayloadSerializer$.serializeExceptionToDse(ExceptionPayloadSerializer.scala:35)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$26(DbfsServerBackend.scala:590)\n\tat com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:473)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:442)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.handleMessageInternal(UnaryGrpcService.scala:130)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$3(UnaryGrpcService.scala:77)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$1(UnaryGrpcService.scala:77)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ExecutionError</span>: An error occurred while calling o465.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:207)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:105)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1387)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1413)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1407)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1093)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1093)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$27(DbfsServerBackend.scala:592)\n\tat com.databricks.backend.daemon.data.exceptionpayload.ExceptionPayloadSerializer$.serializeExceptionToDse(ExceptionPayloadSerializer.scala:35)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$26(DbfsServerBackend.scala:590)\n\tat com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:473)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:442)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.handleMessageInternal(UnaryGrpcService.scala:130)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$3(UnaryGrpcService.scala:77)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$1(UnaryGrpcService.scala:77)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\n[Trace ID: 00-70b7cedf14c79226a7b9656fc0a4bc40-571e4495e7078394-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-5432627103706474>, line 15\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# # Unmount (only if you are sure)\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# dbutils.fs.unmount(\"/mnt/raw-data\")\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Then remount with new config\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(\n\u001B[1;32m     16\u001B[0m     source\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwasbs://transformed-data@crmstorageaccountyt.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     mount_point\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/transformed-data\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     18\u001B[0m     extra_configs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     19\u001B[0m        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.azure.account.key.crmstorageaccountyt.blob.core.windows.net\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     20\u001B[0m            dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricksScope\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecretskv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m     }\n\u001B[1;32m     22\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:172\u001B[0m, in \u001B[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    170\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    171\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
        "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o465.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.send0(BaseDbfsClient.scala:207)\n\tat com.databricks.backend.daemon.data.client.BaseDbfsClient.sendIdempotent(BaseDbfsClient.scala:105)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1387)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1413)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:104)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:104)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:104)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$recordDbutilsFsOp$6(DBUtilsCore.scala:186)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:186)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1407)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/transformed-data\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1093)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1093)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$27(DbfsServerBackend.scala:592)\n\tat com.databricks.backend.daemon.data.exceptionpayload.ExceptionPayloadSerializer$.serializeExceptionToDse(ExceptionPayloadSerializer.scala:35)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handlers$26(DbfsServerBackend.scala:590)\n\tat com.databricks.rpc.armeria.JettyCompatibilityWrapperBlocking.$anonfun$unaryRpcHandler$1(CompatServerBackend.scala:473)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler$.$anonfun$blocking$2(UnaryRpcHandler.scala:442)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$callFunc$2(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.callFunc(UnaryRpcHandler.scala:314)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFuncWithHooks(UnaryRpcHandler.scala:647)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.$anonfun$callFunc$3(UnaryRpcHandler.scala:625)\n\tat com.databricks.rpc.OperationSpan.$anonfun$wrapFuture$1(OperationSpan.scala:66)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.OperationSpan.withAttributionContext(OperationSpan.scala:20)\n\tat com.databricks.rpc.OperationSpan.wrapFuture(OperationSpan.scala:65)\n\tat com.databricks.rpc.armeria.UnaryRpcHandlerInternal.callFunc(UnaryRpcHandler.scala:623)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$3(UnaryRpcHandler.scala:274)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.withAttributionContext(UnaryRpcHandler.scala:43)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc0$2(UnaryRpcHandler.scala:234)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc0(UnaryRpcHandler.scala:234)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.$anonfun$handleRpc$1(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.server.internal.RequestCompletionTracker.wrap(RequestCompletionTracker.scala:184)\n\tat com.databricks.rpc.armeria.UnaryRpcHandler.handleRpc(UnaryRpcHandler.scala:205)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.handleMessageInternal(UnaryGrpcService.scala:130)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$3(UnaryGrpcService.scala:77)\n\tat com.databricks.rpc.armeria.Util$.handleUnexpectedExceptions(Util.scala:84)\n\tat com.databricks.rpc.armeria.UnaryGrpcService.$anonfun$handleMessage$1(UnaryGrpcService.scala:77)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:46)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:46)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:77)\n\tat com.databricks.threading.DatabricksExecutionContext$InstrumentedRunnable.run(DatabricksExecutionContext.scala:36)\n\tat grpc_shaded.com.linecorp.armeria.common.DefaultContextAwareRunnable.run(DefaultContextAwareRunnable.java:45)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$2(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n\tat com.databricks.threading.ContextBoundRunnable.withAttributionContext(ContextBoundRunnable.scala:7)\n\tat com.databricks.threading.ContextBoundRunnable.$anonfun$run$1(ContextBoundRunnable.scala:16)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.threading.ContextBoundRunnable.run(ContextBoundRunnable.scala:15)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$2(InstrumentedExecutorService.scala:257)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$instrumentationWrapper$1(InstrumentedExecutorService.scala:299)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.threading.InstrumentedExecutorService.trackActiveThreads(InstrumentedExecutorService.scala:72)\n\tat com.databricks.threading.InstrumentedExecutorService.instrumentationWrapper(InstrumentedExecutorService.scala:287)\n\tat com.databricks.threading.InstrumentedExecutorService.$anonfun$makeContextAware$1(InstrumentedExecutorService.scala:259)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Unmount (only if you are sure)\n",
    "# dbutils.fs.unmount(\"/mnt/raw-data\")\n",
    "\n",
    "# # Then remount with new config\n",
    "# dbutils.fs.mount(\n",
    "#     source=\"wasbs://raw-data@crmstorageaccountyt.blob.core.windows.net\",\n",
    "#     mount_point=\"/mnt/raw-data\",\n",
    "#     extra_configs = {\n",
    "#        \"fs.azure.account.key.crmstorageaccountyt.blob.core.windows.net\":\n",
    "#            dbutils.secrets.get(\"databricksScope\",\"secretskv\")\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# Then remount with new config\n",
    "dbutils.fs.mount(\n",
    "    source=\"wasbs://transformed-data@crmstorageaccountyt.blob.core.windows.net\",\n",
    "    mount_point=\"/mnt/transformed-data\",\n",
    "    extra_configs = {\n",
    "       \"fs.azure.account.key.crmstorageaccountyt.blob.core.windows.net\":\n",
    "           dbutils.secrets.get(\"databricksScope\",\"secretskv\")\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6411f0ca-5802-4439-86ff-b9a6e54121c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/databricks-datasets -> databricks-datasets\n/Volumes -> UnityCatalogVolumes\n/databricks/mlflow-tracking -> databricks/mlflow-tracking\n/mnt/raw-data -> wasbs://raw-data@crmstorageaccountyt.blob.core.windows.net\n/databricks-results -> databricks-results\n/databricks/mlflow-registry -> databricks/mlflow-registry\n/mnt/transformed-data -> wasbs://transformed-data@crmstorageaccountyt.blob.core.windows.net\n/Volume -> DbfsReserved\n/volumes -> DbfsReserved\n/ -> DatabricksRoot\n/volume -> DbfsReserved\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/raw-data/accounts.csv</td><td>accounts.csv</td><td>4670</td><td>1764567390000</td></tr><tr><td>dbfs:/mnt/raw-data/data_dictionary.csv</td><td>data_dictionary.csv</td><td>996</td><td>1764567405000</td></tr><tr><td>dbfs:/mnt/raw-data/products.csv</td><td>products.csv</td><td>171</td><td>1764567448000</td></tr><tr><td>dbfs:/mnt/raw-data/sales_pipeline.csv</td><td>sales_pipeline.csv</td><td>637773</td><td>1764567432000</td></tr><tr><td>dbfs:/mnt/raw-data/sales_teams.csv</td><td>sales_teams.csv</td><td>1284</td><td>1764567465000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/raw-data/accounts.csv",
         "accounts.csv",
         4670,
         1764567390000
        ],
        [
         "dbfs:/mnt/raw-data/data_dictionary.csv",
         "data_dictionary.csv",
         996,
         1764567405000
        ],
        [
         "dbfs:/mnt/raw-data/products.csv",
         "products.csv",
         171,
         1764567448000
        ],
        [
         "dbfs:/mnt/raw-data/sales_pipeline.csv",
         "sales_pipeline.csv",
         637773,
         1764567432000
        ],
        [
         "dbfs:/mnt/raw-data/sales_teams.csv",
         "sales_teams.csv",
         1284,
         1764567465000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all mounts (look for /mnt/raw-data)\n",
    "for m in dbutils.fs.mounts():\n",
    "    print(m.mountPoint, \"->\", m.source)\n",
    "\n",
    "# list the content to confirm mount works\n",
    "try:\n",
    "    display(dbutils.fs.ls(\"/mnt/raw-data\"))\n",
    "except Exception as e:\n",
    "    print(\"ls error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a814b9-244e-439e-a2a6-3a84fa20c75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/raw-data/accounts.csv', name='accounts.csv', size=4670, modificationTime=1764567390000),\n",
       " FileInfo(path='dbfs:/mnt/raw-data/data_dictionary.csv', name='data_dictionary.csv', size=996, modificationTime=1764567405000),\n",
       " FileInfo(path='dbfs:/mnt/raw-data/products.csv', name='products.csv', size=171, modificationTime=1764567448000),\n",
       " FileInfo(path='dbfs:/mnt/raw-data/sales_pipeline.csv', name='sales_pipeline.csv', size=637773, modificationTime=1764567432000),\n",
       " FileInfo(path='dbfs:/mnt/raw-data/sales_teams.csv', name='sales_teams.csv', size=1284, modificationTime=1764567465000)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/raw-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0b9f3b-72a3-4efd-9db0-ba34d22df63c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/transformed-data\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc48264d-30c6-47b1-84aa-fafb1d09a8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----------------+-------+---------+---------------+-------------+\n|         account|   sector|year_established|revenue|employees|office_location|subsidiary_of|\n+----------------+---------+----------------+-------+---------+---------------+-------------+\n|Acme Corporation|technolgy|            1996|1100.04|     2822|  United States|         NULL|\n|      Betasoloin|  medical|            1999| 251.41|      495|  United States|         NULL|\n+----------------+---------+----------------+-------+---------+---------------+-------------+\nonly showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "accounts_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/raw-data/accounts.csv\")\n",
    "data_dictionary_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/raw-data/data_dictionary.csv\")\n",
    "products_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/raw-data/products.csv\")\n",
    "sales_pipeline_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/raw-data/sales_pipeline.csv\")\n",
    "sales_teams_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/mnt/raw-data/sales_teams.csv\")\n",
    "\n",
    "accounts_df.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7137405e-d31c-4048-a7ab-33d879c33937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of']\n['Table', 'Field', 'Description']\n['product', 'series', 'sales_price']\n['opportunity_id', 'sales_agent', 'product', 'account', 'deal_stage', 'engage_date', 'close_date', 'close_value']\n['sales_agent', 'manager', 'regional_office']\n"
     ]
    }
   ],
   "source": [
    "print(accounts_df.columns)\n",
    "print(data_dictionary_df.columns)\n",
    "print(products_df.columns)\n",
    "print(sales_pipeline_df.columns)\n",
    "print(sales_teams_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d4cc83-e5f0-4795-9ad0-aa06cd4629d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----------------+-------+---------+---------------+--------------+\n|         account|   sector|year_established|revenue|employees|office_location|parent_company|\n+----------------+---------+----------------+-------+---------+---------------+--------------+\n|Acme Corporation|technolgy|            1996|1100.04|     2822|  United States|          NULL|\n+----------------+---------+----------------+-------+---------+---------------+--------------+\nonly showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "accounts_df = accounts_df.withColumnRenamed(\"subsidiary_of\", \"parent_company\")\n",
    "data_dictionary_df = data_dictionary_df.withColumnRenamed(\"Table\", \"table\").withColumnRenamed(\"Field\", \"field\").withColumnRenamed(\"Description\", \"description\")\n",
    "\n",
    "accounts_df.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48304f9b-cad3-4329-b5f5-6774611c2890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+\n|   table|  field| description|\n+--------+-------+------------+\n|accounts|account|Company name|\n+--------+-------+------------+\nonly showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "data_dictionary_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a975b3-affa-4575-8168-04d5e7e8b0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------------+-------+---------+---------------+--------------+\n|account|sector|year_established|revenue|employees|office_location|parent_company|\n+-------+------+----------------+-------+---------+---------------+--------------+\n|      0|     0|               0|      0|        0|              0|            70|\n+-------+------+----------------+-------+---------+---------------+--------------+\n\n+-----+-----+-----------+\n|table|field|description|\n+-----+-----+-----------+\n|    0|    0|          0|\n+-----+-----+-----------+\n\n+-------+------+-----------+\n|product|series|sales_price|\n+-------+------+-----------+\n|      0|     0|          0|\n+-------+------+-----------+\n\n+--------------+-----------+-------+-------+----------+-----------+----------+-----------+\n|opportunity_id|sales_agent|product|account|deal_stage|engage_date|close_date|close_value|\n+--------------+-----------+-------+-------+----------+-----------+----------+-----------+\n|             0|          0|      0|   1425|         0|        500|      2089|       2089|\n+--------------+-----------+-------+-------+----------+-----------+----------+-----------+\n\n+-----------+-------+---------------+\n|sales_agent|manager|regional_office|\n+-----------+-------+---------------+\n|          0|      0|              0|\n+-----------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1 NULL COUNTS FOR accounts_df\n",
    "null_counts_accounts_df = accounts_df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in accounts_df.columns]\n",
    ")\n",
    "\n",
    "# 2 NULL COUNTS FOR data_dictionary_df\n",
    "null_counts_data_dictionary_df = data_dictionary_df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in data_dictionary_df.columns]\n",
    ")\n",
    "\n",
    "# 3 NULL COUNTS FOR products_df\n",
    "null_counts_products_df = products_df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in products_df.columns]\n",
    ")\n",
    "\n",
    "# 4 NULL COUNTS FOR sales_pipeline_df\n",
    "null_counts_sales_pipeline_df = sales_pipeline_df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in sales_pipeline_df.columns]\n",
    ")\n",
    "\n",
    "# 5 NULL COUNTS FOR sales_teams_df\n",
    "null_counts_sales_teams_df = sales_teams_df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in sales_teams_df.columns]\n",
    ")\n",
    "\n",
    "# SHOW RESULTS\n",
    "null_counts_accounts_df.show()\n",
    "null_counts_data_dictionary_df.show()\n",
    "null_counts_products_df.show()\n",
    "null_counts_sales_pipeline_df.show()\n",
    "null_counts_sales_teams_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6133c50-e76d-407b-883e-f3ec7b9c10a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accounts_df = accounts_df.fillna({\n",
    "    \"parent_company\": \"Independent\"\n",
    "})\n",
    "\n",
    "sales_pipeline_df = sales_pipeline_df.fillna({\n",
    "    \"account\": \"unknown\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2154b0f4-7409-4157-8e45-9059068b0b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>account</th><th>sector</th><th>year_established</th><th>revenue</th><th>employees</th><th>office_location</th><th>parent_company</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "account",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sector",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "year_established",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employees",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "office_location",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "parent_company",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "null_counts_accounts_df = accounts_df.select([sum(when(col(column).isNull(), 1).otherwise(0)).alias(column) for column in accounts_df.columns])\n",
    "null_counts_accounts_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6576d5-e931-42df-b0e4-1db2b3de863b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accounts_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/mnt/transformed-data/accounts\")\n",
    "data_dictionary_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/mnt/transformed-data/data_dictionary\")\n",
    "products_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/mnt/transformed-data/products\")\n",
    "sales_pipeline_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/mnt/transformed-data/sales_pipeline\")\n",
    "sales_teams_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/mnt/transformed-data/sales_teams\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7407247b-474d-4191-abb9-1a31b968eeb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sale Data Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}